
\chapter{A generic sound processing library}

\epigraph{Numbers it is. All music when you come to think.  Two
  multiplied by two divided by half is twice one.  Vibrations: chords
  those are. One plus two plus six is seven. Do anything you like with
  figures juggling. Always find out this equal to that. Symmetry under
  a cemetery wall. He doesn’t see my mourning. Callous: all for his
  own gut. Musemathematics. And you think you’re listening to the
  etherial. But suppose you said it like: Martha, seven times nine
  minus $x$ is thirtyfive thousand.  Fall quite flat. It’s on account of
  the sounds it is.}{\emph{Ulysses}\\\textsc{James Joyce}}

\todo{Este capítulo en general necesita más dibujos.}

\section{Analysis}

Requirements \ref{req:iter1-begin} to \ref{req:iter1-end} and
\ref{req:iter1-begin2} to \ref{req:iter1-end2} refer to the second
layer of our system --- in a bottom-up approach. The most crucial
question here: how do we represent a sound in a computer? Then, a new
question arises: how do we get the sound to the speakers? The later
question has a trivial answer --- use whatever API your operating
system exposes for delivering sound to the soundcard --- but the first
question is still to be answered. Actually, the solution to this first
question mostly subsumes the issue of how to interface with these
external interfaces, thus, shall we debate it with care.

\subsection{Factors of sound representation}
\todo{Me he dado cuenta de que esta sección introduce muchas
  definiciones. ¿Tal vez utilizar un estilo más formal usando el
  paquete theorem para las definiciones ayudaría a usar el documento
  como referencia?}

A sound signal is a longitudinal wave of air in motion. We can
analogically record the \emph{proximal stimuli} --- i.e. the physical
stimuli leading to the subjective act of perception
\cite{goldstein01sensation} --- sound by measuring the successive
oscillating values of air pressure in the observer's point in
space. Note that an static air pressure value can not be perceived and
the sensation of sound is caused by the relative oscillation of this
measure. The \emph{amplitude} of this change is associated to our
perception of \emph{loudness}, the frequency of this oscillation
mostly logarithmically determines our perception of pitch. We phrased
this conditionally because these two variables are actually
interrelated and our actual subjective perception of loudness might
vary with pitch and otherwise \cite{fletcher37loudness}.

Most of the time, we represent the relative air pressure value as a
voltage, that varies at some range --- p.e. $[ -5, 5 ] V$. This is an
analogous signal that we have to discretise somehow in order to
manipulate it computationally.

\subsubsection{Temporal quantisation}

Temporal quantisation relates to how many times per second do we store
the current voltage or air pressure value. The value of the signal
between to equally spaced in time samples is unknown, but we can use
some interpolation method to \emph{upsample} a signal --- i.e. to
figure out what is between two samples. Most of the time we refer to
the \emph{sampling rate}, in hertz, as the frequency of the temporal
quantisation.

We know from \emph{Niquist-Shannon sampling theorem} that perfect
reconstruction of a signal is possible when the sampling frequency is
greater than twice the maximum frequency of the signal being sampled,
or equivalently, when the \emph{Nyquist frequency} (half the sample
rate) exceeds the highest frequency of the signal being
sampled. Because the hearing range in most human beings is 20 Hz--20
kHz, audio compact discs use a 44.1 kHz sampling rate. Other popular
rates in audio production are 48 kHz, 96 kHz and 192 kHz. Sampling
rates bellow 44.1 kHz are used also in old computer games that were
limited by the computing power and low bandwidth systems such as
telephone, where low cost and proper understanding of human speech is
more important than audio fidelity.

The sound representation mechanism itself does not vary with the
sampling rate, and thus supporting various rates depends more on the
implementation of the signal processing units and the overall
performance of the system, with the CPU being able to process so many
samples per second being the biggest constraint.

\subsubsection{Spatial quantisation}

Spatial quantisation determines how many possible values can a sample
take in our finite and discrete scale. In a computer system, this is
determined by the size in bits of the underlying type used to store
the samples. An audio CD uses a \emph{bitdepth} of 16 bit with samples
that can take $65.536$ possible values while professional audio uses 24
bit or 32 bit samples. We can even see systems using 64 bit samples
during the processing to avoid accumulative rounding problems due to
heavy arithmetic. The \emph{dynamic range} of a signal with $Q$-bits
quantisation is:
\begin{equation}
  \mathrm{DR_{ADC}} = 20 \times \log_{10}(2^Q) = (6.02 \cdot Q)\, \mathrm{dB}
\end{equation}
The maximum \emph{signal-to-noise} ratio for such a system is:
\begin{equation}
  \mathrm{SNR_{ADC}} =  \left (1.76 + 6.02 \cdot Q \right )\ \mathrm{dB}
\end{equation}

Most analog systems are said to have a dynamic range of around 80 dB
\cite{fries05digital}. Digital audio CD have a theoretical
dynamic range of 96 db --- actual value is around 90 db due to
processing. Human hearing pain level is at 135 db but actually
prolonged exposure to such loud sound can cause damage. A loud rock
concert is around 120 dB and a classical music performance is at
110 db \cite{ludwig09music}, thus requiring bitdepth of at least 24
bit (theoretical dynamic range of 144 dB) for perfect fidelity.

There are some other aspects related to representation of samples in a
computer, such as the \emph{signedness} of the underlying type. Signed
types are usually considered more convenient for audio signals as 0
can be easily recognised as the still no-sound value simplifying
computations. Another important factor is whether we use \emph{fixed
  point} or \emph{floating point} arithmetic. While fixed point is
used in low-cost DSP hardware, floating point is the most common
representation in current audio software as nowadays processors are
optimised for SIMD\footnote{Single Instruction Multimple Data, as
  supported by MMX, 3D Now! and SSE extensions in Intel and AMD chips}
floating point arithmetic. Moreover, the algorithms implementation is
much harder to encode because products produce greater values and
there are many ways on how to account the carry.  Actually, even while
the actual bitdepth (the bit for the mantissa) of a 32 bit floating
point is the same of a 24 bit fixed point, then a 32 bit fixed point
will have a quite lower \emph{quantization error}, but the dynamic
range and SNR of a floating point is much higher because the
values are spaced logarithmically over a huge range
\cite{smith02dsp}. Another factor is the \emph{endianess} of fixed
point values but is relevant only when interfacing with file formats
and output devices.

\subsubsection{Channel space}

Because our hearing system is dicotomically symmetric, audio
engineers discovered that much better fidelity can be achieved by
reproducing the sound with some differences from two separate
loudspeakers. This is the well-known \emph{stereophonic} sound,
commonly named just \emph{stereo}.

For representing such a signal, two different streams of information
are needed for the left and right channels. Moreover, nowdays
\emph{quadraphonic}, and \emph{surround} sound with varying numbers of
channels up to 20.2 are used in different systems.

We call \emph{channel space} to the set of semantic channels we use in
some sort of audio representation --- p.e. stereo audio has channel
space with $left$ and $right$ elements. We use the term
\emph{frame} to call a set of samples coincident in time, this is, the
samples of the various channels at a given time point. Thus, we will
use most of the time the more accurate term \emph{frame rate}.

This rises the problem on how to linearise the multi channel data. The
most common mechanism in domestic hardware is by \emph{interleaving}
the samples of different channels, this is, by storing the frames
sequentially. However, high-end hardware often accepts data in
non-interleaved form where the samples of each channel is stored in a
separate sequence. In this document, we borrow from the image
processing world the term \emph{planar} to refer to non-interleaved
data. Software doing a lot of processing of the audio signal often
chooses this representation as it is easier to scale to varying number
of channels and split the signal to do per-channel filtering. Figure
\ref{fig:interleaving} compares visually the interleaved and planar
formats.

\begin{figure}[h]
  \centering
  \subfloat[]{\includegraphics[width=2in]{pic/fmt-planar.png}}\;
  \subfloat[]{\includegraphics[width=3in]{pic/fmt-interleaved.png}}
  \caption{Multi-channal data in planar (a) and interleaved (b) form.}
  \label{fig:interleaving}
\end{figure}

Another issue is the order in which the data from different semantic
channels is stored. We call a \emph{channel layout} a bijection $L : C
\rightarrow \mathbb{Z}_{\|C\|}$, where $C$ is a given \emph{channel
  space}. For example, the mapping $\{ left
\mapsto 0, right \mapsto 1 \}$ is a common layout for stereo sound,
but $\{ left \mapsto 1, right \mapsto 0 \}$ is sometimes used too.

\subsection{Common solutions}

As we have already noticed, 32 bit floating point sound with planar
left-right layout the most common in software of our kind during
internal processing. As most of this software is written in C, a
simple \texttt{float**} does the job. This was, actually, the internal
representation used in GNU Psychosynth in versions prior to 0.2.0,
wrapped in the \texttt{audio\_buffer} class.

However, this design starts to wobble whenever one has to interface
with some other library or hardware using a different format. Thus,
the \texttt{audio\_buffer} class provided different
\texttt{interleave\_*} and \texttt{deinterleave\_*}, where the
asterisk can be substituted by different sample formats like
\texttt{s16} or \texttt{s32} (fixed point signed 16 bit and 32 bit
respectively). This is very inconvenient because, as we have seen
through this section, many orthogonal factor affect audio
representation inducing a combinatorial explosion of format conversion
functions. Take a look at the 64 different read and write functions in
the \texttt{pcm.c} file of the
LibSndfile\footnote{\url{http://www.mega-nerd.com/libsndfile/}}
library.

This is a maintenance hell, but using the common means for abstracting
orthogonal behaviour variability, i.e. dynamic polymorphism, is simply
not an option in any audio software which supports real-time operation.

\subsection{A generic approach: Boost.GIL}

However, there is a piece of software that proved that this issue can be
solved in C++ using static polymorphism. This is the Generic
Image Library\footnote{http://stlab.adobe.com/gil} which was developed
by Ludovic Cortés et. Al inside Adobe Software Technology Lab that was
later include inside the Boost library distribution.

While sound and image manipulation are quite different, specially from
the psycho-perceptive point of view, they are both a signal processing
problem and thus share a lot in the representational issue. By
realising of a proper conceptual mapping between both worlds (table
\ref{tab:gilmap}), most of the library design and even quite a lot of
code of Boost.GIL can be reused to build a unique state-of-the-art
sound processing library that addresses the aforementioned issues in
an orthogonal generic manner while maintaining near-optimal
performance.

\begin{table}[h]
  \centering
  \begin{tabular}{c|c}
    Boost.GIL & Psynth.Sound \\ \hline\hline
    Channel   & Sample \\
    Color     & Channel \\
    Color Space & Channel Space \\
    Color Layout & Channel Layout \\
    Pixel & Frame \\
    View & Range \\
    Image & Buffer
  \end{tabular}
  \caption{Terminology map from \texttt{boost::gil} to \texttt{psynth::sound}}
  \label{tab:gilmap}
\end{table}

An \emph{image} is bidimensional matrix of \emph{pixels}, that capture
the properties of light electromagnetic waveform at those discrete
points. Each pixel, however, is decomposed in several \emph{colors}
that, for example, capture the intensity in the red, green and blue
sensors of a CCD camera. As there are different ways of decomposing an
audio frame (p.e, stereo, surround, etc.), there are different ways of
decomposing a pixel into several values, known as the \emph{color
  space} (p.e, RGB, CMYK, YUV, etc.). Boost.GIL uses the term
\emph{channel} to name the individual value of one those color
components.

In our audio framework, a \emph{buffer} is unidimensional array of
\emph{frames} that represent a sound or part of a sound --- sound is
continuous and thus we usually process it in chunks. The reader might
note that the the data in a buffer being arranged along the
\emph{time} dimension while the dimensions of an image represent
\emph{physical space} makes these entities completely different from
the processing point of view. However, they share most representation
problems, with sound representation being actually a sub-problem of
image representation, as we have one dimension less. The samples in a
series of audio frames can be stored in an interleaved or planar
fashion as happens with the channels of a pixel. Also, both channels
and samples can vary in signedness, fixed/floating point, bitdepth,
etc.

Those already familiar with Boost.GIL can thus already understand
easily our Psynth.Sound module design and implementation that we are
to describe in the following section.

\section{Design}

\subsection{Core techniques}

The Boost.GIL and thus the Psynth.Sound modules design makes heavy use
of static polymorphism and generic programming via C++ templates to
achieve generality without runtime overhead. We are going to introduce
advanced techniques used in generic programming for the reader
unfamiliar with this programming paradigm.

\subsubsection{Concepts}
\label{sec:concepts}.

\emph{Concepts} \cite{jarvi10concept} are to generic programming what
\emph{interfaces} --- pure abstract classes in C++ --- are to object
oriented programming: they specify the requirements on some
type. However there, are few substantial differences. (1) While
interfaces can only specify the method signatures of its instances, a
concept can specify most syntactic constraints on a type, like the
existence of free functions, operators, nested types, etc. (2) While
dispatching through interfaces requires, at least, a dereference,
addition and function call \cite{driesen96direct}, when using concepts
the concrete function to be executed can be determined and even
inlined at compile-time. (3) One can not declare that a type satisfies
an interface separately from the type definition, but one can say that
a type models a concept at any point of the program. (4) Thus, no
primitive type defines any virtual interface, but one can turn any
primitive type into an instance of any concept via a
\texttt{concept\_map}. (5) Actually, the syntactic properties defined
by a concept its models may differ, but they are matched via the
\texttt{concept\_map}. In fact, C++ concepts are more similar to
Haskell \emph{type classes}, with \texttt{instace} doing the job of
\texttt{concept\_map} \cite{bernardy08comparison}.

Concepts are an extension to the template mechanism to add type
checking for it. In fact, checking and dispatching on requirements can
be achieved with techniques like SFINAE (Substitution Failure Is Not
an Error) \cite{vandervoorde08templates}. Property (5) of our concepts
can be simulated with \emph{traits} \cite{c++traits}. However, both
compiler errors and the code using templates without concepts is
usually much more unreadable.

The proposal of adding concepts to the C++ language was rejected last
year by the standardisation committee and thus we can not use them in
our code. However, Boost.GIL is very influeced by Alexander Stepanov's
deductive approach to computer programming using generic programming
and modeling with concepts, that he elegantly describes in his
master-piece ``Elements of Programming''
\cite{stepanov09elements}. Actually Stepanov worked several years in
Adobe where he held a course ``Foundations of Programming'' based on
his book. Thus, the \emph{modeling} of the library extensively uses
concepts. Its implementation uses a limited form of concept checking
via the Boost.ConceptCheck\footnote{
  Boost.ConceptCheck: \url{http://www.boost.org/doc/libs/release/libs/concept_check/concept_check.htm}}
\cite{siek00concept} library, however, enabling this library in
release mode can affect performance and its syntax is quite more
cumbersome than the concepts in the C++ standard proposal. For
consistency with the Boost.GIL documentation we will use the concept
syntax proposed in the proposal N2081 to the standardisation committee
\cite{gregor06concept}.

The following example defines a concept that is satisfied by every
type that has an \texttt{operator<}:

\begin{lstlisting}
concept LessThanComparable<typename T> {
  bool operator< (T, T);
};
\end{lstlisting}

This allows us to write a generic function that depends on the
existence of a less-than comparator for the parametrised type:

\begin{lstlisting}
template<LessThanComparable T>
const T& min (const T& x, const T& y) {
  return x < y? x : y;
}
\end{lstlisting}

An alternative syntax for specifying that \texttt{T} must satisify the
\texttt{LessThanComparable} concept is the \texttt{where} clause:

\begin{lstlisting}
template<typename T>
    where LessThanComparable<T>
const T& min (const T& x, const T& y) ...
\end{lstlisting}

In fact, this is the only valid syntax when the concept affects
multiple types. Also, the \texttt{where} clause can be used inside
concept definitions to provide specialisation.

Specifying that a type models a concept is done with the
\texttt{concept\_map} device. If the type naturally models the
concept, we can just use:

\begin{lstlisting}
concept_map LessThanComparable<int> {}
\end{lstlisting}

Note that these trivial concept mappings can be avoided by using the
\texttt{auto} keyword in front of the \texttt{concept} keyword in the
concept definition. However, it might happen that a type requires some
wrapping to satisfy the concept. We can do this in the concept map
definition itself.

\begin{lstlisting}
concept_map LessThanComparable<char*> {
  bool operator< (char* a, char* b) {
    return strcmp (a, b) < 0;
  }
}
\end{lstlisting}

Note that this last piece of code is an example of a bad usage of
concept maps, as this specialises the mapping for pointers changing
the expected semantics.

This should suffice as an introduction to concepts in order to
understand the concept definitions that we will later show when
modelling our system. A more detailed view can be read in the cited
bibliography, with \cite{jarvi10concept} being the most updated and
useful from a programmer point of view.

\subsubsection{Metaprogramming}

The C++ template system is Turing complete
\cite{veldhuizen03templates}, thus it can be used to perform any
computation at \emph{compile time}. This was first noted in 1994 by
Erwin Unruh who, in the middle of a C++ standardisation committee,
wrote a template meta-program that outputted the first $N$ prime
numbers on the console using compiler errors \cite{unruh94prime}. Even
though this might seem just a crazy puzzle game, it can be used in
practise and actually new Boost libraries use it extensively. A very
gentle introduction to template metaprogramming can be found in
\cite{alexandrescu01modern}, where Alexandrescu uses them to
instantiate design patterns as generic C++ libraries. A deeper
reference is Abraham's \cite{abrahams04meta}, which focuses on the
Boost Metaprogramming Library\footnote{The Boost.MPL:
  www.boost.org/doc/libs/release/libs/mpl} and introduces the usage of
metaprogramming for building Embedded Domain Specific Languages (EDSL)
in C++. This Boost.MPL, providing reusable meta data structures and
algorithms, is the de-facto standard library for template
metaprogramming\footnote{It is often called ``the STL of template
  metaprogramming''.} and we will use it in our implementation.

Template metaprogramming is possible thanks to \emph{partial template
  specialisation}, that allows giving an alternate definition for a
pattern matched subset of its possible parameter values. A
\emph{metafunction} is thus just a template \texttt{class} or
\texttt{struct} with a public member that holds the result of the
function. It is up to the programmer to choose the naming convention
for the result members of the metafunctions, in the following, we will
use Abraham's style calling \texttt{type} for result values that are a
type, and \texttt{value} for integral values. Listing \ref{lst:fib}
illustrates how can we write and use a metafunction for computing the
$n$-th Fibonacci number.

\begin{lstlisting}[float=h!, 
  caption=Metaprogram for computing the Nth Fibonacci number,
  label=lst:fib]
template <int N>
struct fib {
  enum { 
    value = fib<N-1>::value + fib<N-2>::value; 
  };
};

template <>
struct fib <0> {
  enum { value = 0 };
};

template <>
struct fib <1> {
  enum { value = 1 };
};

int main () {
  return fib<42>::value;
}  
\end{lstlisting}

The program returns the forty-second Fibonacci value. However, it will
take no time to execute, because the number is computed at compile
time. We use recursion to define the metafunction for the general case
and the specialise for the base cases.

If we consider the template system as a meta-language on its own, we
should describe its most outstanding semantic properties. It is a pure
functional programming language, because variables are immutable. It
is lexically scoped. It supports both lazy and strict evaluation,
depending on whether we choose to access the nested \texttt{type}
result name at call site or value usage type. When we look at the meta
type system, we find three meta types: types (which are duck-typed
records), integrals (e.g. \texttt{int}, \texttt{char}, \texttt{bool}
...) and meta-functions (i.e. templates).

The fact that records are duck typed but integrals and metafunctions
cause several inconveniences in practice, specially when dealing with
the later. For example, in the absence of template aliases, returning
a metafunction produced by another function requires defining a nested
struct that inherits from the actual synthesised value. Also, the
template signature should be specified on a template parameter
expecting a template.

In order to simplify our meta type system we shall wrap constants in a
type like on listing \ref{lst:integral_c}.

\begin{lstlisting}[float, 
  caption=Integral constant nullary metafunction  wrapper.,
  label=lst:integral_c]
template <typename T, T V>
struct integral_c
{
  BOOST_STATIC_CONSTANT(T, value = V);
  typedef integral_c<T, V> type;
};
\end{lstlisting}

There are a couple of issues regarding this definition worth
explaining. First, the \texttt{BOOST\_STATIC\_CONSTANT} macro is used
to define a constant. Internally, it will try to use \texttt{enum} or
any other mechanism available to actually define the constant such
that the compiler is not tempted to allocate static memory for the
constant. Second, the \texttt{typedef} referring to itself turns a
constant value into a self returning nullary meta-function. This can
be very convenient because, for example, it allows using
\emph{value}\texttt{::type::value} always on the value usage point,
allowing the caller or producer of the value to choose whether he
wants to evaluate the value lazily.

Because we just wrapped values into a type, we can simplify our
conventional definition of \emph{metafunction}: a meta-function is any
type --- template or not --- that has a nested type called
\texttt{type}.

Now we should also turn metafunctions into first class entities of the
meta-language. We just add a new level of indirection and define a
\emph{metafunction class} as a type with a nested template
metafunction called \texttt{apply}. The example in listing
\ref{lst:high_order_fib} also illustrates the metafunction forwarding
technique when defining the nested \texttt{apply} metafunction by
inheriting from \texttt{fib}.

\begin{lstlisting}[float, caption=Metafunction class for
  computing Fibonacci numbers. We suppose that the previous
  \texttt{fib} definition uses \texttt{integral\_c} to wrap its
  parameters and return types., label=lst:high_order_fib]
struct fib_class { 
   template <class N>
   struct apply : public fib<N> {};
};

int main ()
{
  return fib_class::apply<integral_c<int, 42>>::type::value;
}
\end{lstlisting}

Using this convention the MPL library defines many useful high order
metafunctions that take metafunction classes as input, like
\texttt{mpl::fold} and \texttt{mpl::transform}. Note that it is not
needed to define metafunction classes for all our metafunctions,
instead, we shall convert them when needed using the
\texttt{mpl::quote}\emph{N} functions and the \texttt{mpl::lambda}
facility.

\subsection{Core concepts}

We are now ready to understand the main design and implementation
techniques used in our generic library. Because the library is
\emph{generic}, in the sense of generic programming, most algorithms
and data structures are parametrised such that they can be
instantiated with any concrete type modelling some concepts as we
suggested in section \ref{sec:concepts}. Thus, traditional modelling
techniques like the Unified Modelling Language are not useful since
they are intended for object oriented design.

We are going to follow the following methodology for describing the
library. First, we will name a concept and give a brief description of
its purpose. Then, we will define the concept using the notation
described in section \ref{sec:concepts} and finally we will enumerate
and describe some models for such concept.

For brevity, we will omit basic concepts such as
\texttt{CopyConstructible}, \texttt{Regular}, \texttt{Metafunction},
etc. Their complete definition should be evident and an interested
reader can find most of them in \cite{stepanov09elements}.

\subsubsection{\texttt{ChannelSpaceConcept}}

A channel space is a sequence of whose elements channel tags (empty
types giving a name )

\begin{lstlisting}
concept ChannelSpaceConcept<MPLRandomAccessSequence Cs> 
{};
\end{lstlisting}

Some example models include \texttt{stereo\_space} or
\texttt{surround\_space}. An example on how a user of the library can
define his own channel space follows.

\begin{lstlisting}
struct left_channel {};
struct right_channel {};

typedef mpl::vector<left_channel, right_channel> stereo_space;
\end{lstlisting}

A related trivial concept is
\texttt{ChannelSpaceCompatibleConcept}. Two channel spaces are
compatible if they are the same. In fact, this leaks the underlying
MPL sequence type used in the channel space through the abstraction,
because spaces with the same set of semantic channels might be found
incompatible, but it suffices in practise.

\subsubsection{\texttt{SampleConcept}}

A \emph{sample} is the type we use to represent the amplitude of a
channel at certain point in time.

\begin{lstlisting}
concept SampleConcept<typename T> :
             EqualityComparable<T> {
    typename value_type      = T;
    // use sample_traits<T>::value_type to access it
    typename reference       = T&;
    // use sample_traits<T>::reference to access it
    typename pointer         = T*;
    // use sample_traits<T>::pointer to access it
    typename const_reference = const T&;
    // use sample_traits<T>::const_reference to access it
    typename const_pointer   = const T*;
    // use sample_traits<T>::const_pointer to access it
    static const bool is_mutable;
    // use sample_traits<T>::is_mutable to access it

    static T min_value(); 
    // use sample_traits<T>::min_value to access it
    static T zero_value(); 
    // use sample_traits<T>::zero_value to access it
    static T max_value(); 
    // use sample_traits<T>::min_value to access it
};
\end{lstlisting}

Built-in scalar types like \texttt{char}, \texttt{int} or
\texttt{float} model \texttt{SampleConcept} by default. 

The \texttt{scoped\_sample\_value<Type, Min, Max, Zero>} template
class models the concept whenever \texttt{Type} is a scalar type and
\texttt{Min}, \texttt{Zero} and \texttt{Max} satisfy:
\begin{equation}
Min < Zero < Max \land \forall x \in Type, x + Zero = x
\end{equation}
Note that, in order to avoid the limitation of floating point values
not being usable as template arguments, \texttt{Min}, \texttt{Zero}
and \texttt{Max} should be a type with a static method
\texttt{apply()} that returns the actual value. It should be used to
constraint the ``clipping thresholds'' of floating point types. For
example, the \texttt{bits32sf} model defined as:

\begin{lstlisting}
  scoped_sample<float,
                float_minus_one,
                float_zero,
                float_one>;
\end{lstlisting}

User defined types should specialise \texttt{sample\_traits} to map
the concept.

Related trivial concepts are \texttt{MutableSampleConcept} and
\texttt{SampleValueConcept} (a sample that is also \texttt{Regular}).

\subsubsection{\texttt{SampleConvertibleConcept}}

Because casting does not suffice in most cases, one should
override a \texttt{T sample\_convert (U)} function for \texttt{U} to
be convertible into \texttt{T}.

\begin{lstlisting}
concept SampleConvertibleConcept<SampleConcept SrcSample,
     SampleValueConcept DstSample> {
    DstSample sample_convert (const SrcSample&);
};
\end{lstlisting}

The library provides overrides for \texttt{sample\_convert} making
most supplied sample types being convertible too.

\subsubsection{\texttt{ChannelBaseConcept}}

A \emph{channel base} is a container of channel elements (such as
samples, sample references or sample pointers).

The most common use of channel base is in the implementation of a
frame, in which case the channel elements are sample values. The
channel base concept, however, can be used in other scenarios. For
example, a planar frame has samples that are not contiguous in
memory. Its reference is a proxy class that uses a channel base whose
elements are sample references. Its iterator uses a channel base whose
elements are sample iterators.

\begin{lstlisting}
concept ChannelBaseConcept<typename T> :
      CopyConstructible<T>, EqualityComparable<T>
{
    // a Psynth layout (the channel space and element permutation)
    typename layout;     
        
    // The type of K-th element
    template <int K> struct kth_element_type;
    where Metafunction<kth_element_type>;
    
    // The result of at_c
    template <int K> 
    struct kth_element_const_reference_type;
    where Metafunction<
            kth_element_const_reference_type>;        
    
    template <int K> 
    kth_element_const_reference_type<T,K>::type at_c(T);

    // Copy-constructible and equality comparable 
    // with other compatible channel bases
    template <ChannelBaseConcept T2> 
        where { ChannelBasesCompatibleConcept<T,T2> } 
        T::T(T2);
    template <ChannelBaseConcept T2> 
        where { ChannelBasesCompatibleConcept<T,T2> } 
        bool operator==(const T&, const T2&);
    template <ChannelBaseConcept T2> 
        where { ChannelBasesCompatibleConcept<T,T2> } 
        bool operator!=(const T&, const T2&);
};
\end{lstlisting}

A channel base must have an associated layout (which consists of a
channel space, as well as an ordering of the samples).  There are two
ways to index the elements of a channel base: A physical index
corresponds to the way they are ordered in memory, and a semantic
index corresponds to the way the elements are ordered in their channel
space.  For example, in the stereo channel space the elements are
ordered as \texttt{\{left\_channel, right\_channel\}}. For a channel
base with a RL-stereo layout, the first element in physical ordering
is the right element, whereas the first semantic element is the red
one.  Models of \texttt{ChannelBaseConcept} are required to provide
the \texttt{at\_c<K>(ChannelBase)} function, which allows for
accessing the elements based on their physical order. Psynth provides
a \texttt{semantic\_at\_c<K>(ChannelBase)} function (described
later) which can operate on any model of \texttt{ChannelBaseConcept}
and returns the corresponding semantic element.

The library provides an \texttt{homogeneous\_channel\_base} class that
models the concept.

\subsubsection{\texttt{...Concept}}

\subsection{Going dynamic}
\subsection{Input and Output module}
\subsection{Synthesis module}

\section{Validation}

\subsection{Unit testing}


\subsection{Performance}

\subsection{Integration}

The module was first developed separately from the main development
branch in a branch called \texttt{gil-import}. Once the previous tests
were passed, the former signal representation classes and I/O code was
removed from the code base. The upper layers --- mainly the
\texttt{graph} layer --- was adapted to use the new library. Note
that, given that we will mostly rewrite the \texttt{graph} layer in
the next iteration we tried to make minimal changes to get the project
compile and run properly again.

The system was then \emph{peer reviewed} by project collaborators,
mainly by the maintainer of the Ubuntu/Trinux packages Aleksander
Morgado\footnote{Aleksander Morgado's web blog:
  \url{http://sigquit.wordpress.com}}. After minor bugfixing, we
agreed to make a new Psychosynth 0.2.0 release that included the new
code described in this chapter and some other fixes and modifications
developed alongside.

The official changelog briefing for this release is included in note
\ref{note:changelog02}.

\begin{mynote}[Changelog of Psychosynth 0.2.0]
\label{note:changelog02}
\begin{itemize}
\item New audio processing and I/O subsystem based on template programming
for generic yet efficient sound signals.

\item The extreme latency when using ALSA bug seems to be fixed in
  some cases.

\item No longer depend on libvorbis, libsndfile can now handle ogg and flac
files too.

\item No longer depend on \texttt{libsigc++}, using \texttt{boost::signals}
  which, which is a bit slower but neglibe and this simplifies the
  dependencies.

\item The mouse wheel now scrolls in the object selector.

\item The object selector no longer lets mouse clicks pass through.  

\item Backwards reproducing a sample works a bit better now too.

\item Some new niceties in the framework base layer, including some
  experiments on applying the C3 class linearisation algorithm in raw
  C++.

\item C++0x features are being used in the code. For GCC, this means
  version 4.5 shall be used. We doubt it will compile with any other
  compiler (maybe latest VS), but users are welcomed to try and report.

\item For this same reason, Boost.Threads is no longer a dependency,
  we use STL threads instead.
\end{itemize}
\end{mynote}

\section{Conclusions}

In this iteration we developed a generic approach to representation
and input and output of audio data. We do not have any records of
any audio software using such paradigm in their code, so this
development have been exploratory and has a lot of value in its
novelty. This however delayed our development more than expected in
our original plan.

\subsection{Benefits and caveats}

The three main advantages in the new code are:
\begin{enumerate}
\item Code can be mostly abstracted from the audio format
  representation while retaining near optimal performance. Because
  generality allowed decoupling orthogonal concepts, each audio
  representation factor can be optimised and tuned for computational
  accuracy on its own, leading to higher quality code with lower
  maintenance cost as we avoid the combinatorial explosion that
  happens otherwise.

\item Algorithms correctly written with our generic facilities have a
  performance equivalent to the hand-written code. When a certain
  algorithm has not general interpretation or can not be efficiently
  implemented generally, the library still allows for the algorithm to
  be written with for a concrete or a constrained family of audio
  formats.

\item Because the signal format is encoded in the data type, we can
  either statically check that the data is in the correct format
  through our processing chain, or trivially enforce runtime checks
  when the format is unknown at compile time (via
  \texttt{dynamic\_buffer} and similar tools), leading to more secure
  code.
\end{enumerate}

Also, because a lot of learning have happened since the old code base
was written, the new code is better written and quite safer, making
use of exceptions and scope guards \cite{alexandrescu00gener}.

Even though we believe the benefits outweight the drawbacks, we have
to acknowledge the caveats of our new approach, the most relevant
being:

\begin{enumerate}
\item The new code uses advanced C++ programming techniques that many
  programmers find hard to understand. Thus, it might be harder for
  casual contributors to join the project in the future.

\item In the absence of real language support for concepts, template
  metaprograms leak their implementation in user code's compilation
  errors. This is so because, actually, by expanding the type
  instantiations in the compilation error, the compiler is actually
  showing a full backtrace of the metaprogram. This leads to cryptic
  error messages that often obfuscate the real source of the problem,
  discouraging novel developers.

\item Template metaprograms take longer to compile. However, proper
  usage of the new \texttt{extern template} facility should avoid
  redundantly instantiating templates in different translation units
  only to be discarded by the linker mostly solving this issue. Also,
  because the compiler generates different object code for each audio
  format, thoughtless use of the library can lead to code bloat and
  too large binary size.
\end{enumerate}

\subsection{Future work}

While the current status of the library is quite satisfactory for our
needs, there is still a lot of room for improvement that we will
postpone since they fall outside this project's scope. Nonetheless it
is worth enumerating them:

\subsubsection{Virtual and adapted iterators and ranges}

Boost.GIL included a ``locator adapter'' and ``virtual locator''
notions that allowed creating or modifying images lazily via a
function object. We discarded implementing them because they were
coupled to their \texttt{locator} concept which is specific to the
problem of image representation --- locators are in practice 2D
iterators. Moreover, they used the indexed position in the image as
the parameter to the function object that synthesised the
image. However, because audio is processed in chunks, the position in
the audio buffer is meaningless for the synthesis or filter function
--- instead, some stateful function object which includes a notion of
time position related to the frame rate is needed. Thus, many
unexplored design decisions should be taken, and the interactions with
other similar libraries like Boost.Iterator\footnote{The
  Boost.Iterator Library:
  \url{http://www.boost.org/doc/libs/release/libs/iterator}} and
Boost.Range\footnote{The Boost.Range Library:
  \url{http://www.boost.org/doc/libs/release/libs/range}} should be
carefully evaluated.

\subsubsection{Better arithmetic support}

The library includes some basic arithmetic support for samples. There
are few complications when developing full generic arithmetic support
for samples and frames. As Lubomir Bourdev, lead developer of
Boost.GIL, stated it in an email conversation with us:

\begin{quotation}
``Doing arithmetic operations is tricky for a number of reasons:

\begin{itemize}
\item What do you do on overflow? Clip, throw exception, allow out of
  range values?

\item What is the type to be used during conversion? Even if the
  source and destination have the same type, the operation might need
  to be done in another type and then cast back.

\item Certain arithmetic operations have no meaningful interpretation
  as far as color is concerned, such as multiplying one pixel by
  another. 
\end{itemize}

Because of issues like these we have not tackled the problem of
providing arithmetic operations, but we have provided generic
operations that can be done per channel or per pair of channels which
could be the basis for arithmetic operations.''
\end{quotation}

Nonetheless with time and effort the problem could be
approached making some compromises. Some of the issues Lubomir states
have different answers for sound processing, in fact, allowing out of
range values is the best answer for the first question given the fact
that sound amplitude is not naturally constrained and clipping is
introduced only by the DAC hardware or when moving from floating to a
fixed point representation. Maybe, not all those questions have to be
answered in order to improve the arithmetic support.

One of the main annoyances when writing a generic algorithm is using
the per sample \texttt{static\_*} algorithms. Using them we could
write a simple arithmetic layer for frames that would simplify the
user code. However, even though we do not have experimental data, we
believe this straightforward solution could introduce overhead. This
is because every \texttt{static\_*} unrolls one statement per
channel. Thus, a simple frame arithmetic expression would in fact
result into many sequence points that is yet to be tested whether
compilers can optimise properly.

This is not a dead end. Using the \emph{expressions templates}
\cite{veldhuizen95expression} technique and r-value references we can
perform transformations with the aid of metaprogramming such that
sequence points are not introduced by the arithmetic expression
itself. A expression template framework like Boost.Proto\footnote{The
  Boost Proto library:
  \url{http://www.boost.org/doc/libs/release/libs/proto}}
\cite{niebler07proto} could be of great help. Moreover, with careful
studying of the audio DSL's studied in section \ref{sec:dsl} and the
usage of these same techniques the scope of such effort could be
broadened to build a full sound synthesis and processing EDSL for C++.

\subsubsection{Submission to Boost}

In our conversations with Lubomir Bourdev he suggested submitting our
library for inclusion in the Boost package. However, there are few
issues that we should tackle before that:

\begin{enumerate}
\item A lot of code is algorithmically identical to that of Boost.GIL
  with changes only in terminology. A lot of work in properly
  abstracting such common parts should be made to avoid code
  repetition and doubled maintenance effort inside Boost.

\item As we said earlier, interesting interactions can emerge with
  the Boost.Iterator and Boost.Range libraries. We believe that any
  possible issues and unneeded incompatibilities with those libraries
  should be solved before submission into Boost.

\item Boost is written in C++03 standard, while our code uses
  C++0x. Moreover, our code has dependencies with other submodules in
  \texttt{psynth::base}, specially the expection and logging
  system. While these dependencies are not too strong, the effort made
  to polish these corners is outside the scope of the current project.
\end{enumerate}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "00-main"
%%% End: 
